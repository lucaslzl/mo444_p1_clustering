{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"unicamp.png\" width=\"150\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MO444/MC886 - Aprendizado de Máquina e Reconhecimento de Padrões\n",
    "\n",
    "Esse trabalho foi feito pelos seguintes membros:\n",
    "\n",
    "- Lucas Zanco Ladeira - 188951\n",
    "- Rafael Scherer - 204990\n",
    "\n",
    "O código original deste projeto está disponível em [repository inside Github](https://github.com/lucaslzl/p1_clustering). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Agrupamento e Redução de Dimensionalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Introdução\n",
    "\n",
    "Neste trabalho foi necessário implementar dois modelos de agrupamento e utilizar o algoritmo <b>Principal Component Analysis (PCA)</b> da biblioteca scikit-learn para a tarefa de redução de dimensionalidade. Os modelos implementados compreendem o <b>KMeans</b> e o <b>DBScan</b>. De forma resumida, no primeiro caso são selecionados centróides de clusters e analisados os clusters formados por esses centróides. O algoritmo é iterado um determinado número de vezes e clusters são encontrados na qual a distância entre os membros dos clusters e os centróides seja mínima dentro das iterações. No segundo caso, é calculada a distância entre todos os registros para encontrar quais formam clusters considerando uma distância máxima e uma vizinhança mínima. Os registros são classificados como <i>outlier</i>, borda, e central. Sendo assim, é um algoritmo interessante para identificar <i>outliers</i> dentro de conjuntos de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta seção será descrito o código fonte dos algoritmos implementados. Para tal, será sub-dividia em: a) KMeans, b) DBScan, c) PCA. Mesmo sendo que foi utilizada uma biblioteca para implementar o PCA, o código fonte será disponibilizado neste relatório."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - a) KMeans\n",
    "\n",
    "Esse algoritmo possui dois métodos principais, seguindo o padrão utilizado pela biblioteca scikit-learn, esses compreendem <i>fit</i> e <i>predict</i>. O primeiro tem o intuito de treinar o modelo, ou seja, agrupar os dados em K clusters. <br>\n",
    "Inicialmente, os centroides dos K clusters são inicializados utilizando um de dois métodos disponíveis:\n",
    "- 'forgy': são escolhidos K centroides aleatórios a partir do conjunto de dados.\n",
    "- 'kmeans++', onde são escolhidos K centroides com base em uma lógica probabilística.\n",
    "\n",
    "Após a inicialização dos centroides, é iniciado o agrupamento dos dados em clusters. Para cada ponto pertencente ao conjunto de dados, é calculada a distância entre o ponto e cada centroide definido, em seguida, o ponto é atribuido ao cluster do centroide cuja distância calculada foi mínima.\n",
    "\n",
    "Para fins de análise do comportamento do algoritmo, foi criado um dicionário 'history', que armazena algumas informações relevantes para cada iteração do algoritmo, sendo elas: centroides, pontos pertencentes a cada clusters e inércia.\n",
    "\n",
    "Por fim, o algoritmo conta com duas possíveis condições de parada:\n",
    "- Caso o número de iterações realizadas alcance o limite definido por <i>max_iter</i>\n",
    "- Caso os centroides dos K clusters convirjam, ou seja, parem de se mover em alguma iteração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, data):\n",
    "    # data initialization\n",
    "    self._data_init(data)\n",
    "\n",
    "    for i in range(self.max_iter):\n",
    "        # assign the previous centroids\n",
    "        previous_centroids = dict(self.centroids)\n",
    "\n",
    "        # initialize the clusters dictionary\n",
    "        for label in range(self.k):\n",
    "            self.clusters[label] = []\n",
    "\n",
    "        # assign each data point to a cluster\n",
    "        for data_point in data:\n",
    "            distances = self._centroid_distance(data_point)\n",
    "            # the data point is assigned to the nearest cluster\n",
    "            label = np.argmin(distances)\n",
    "            self.clusters[label].append(data_point)\n",
    "\n",
    "        # the position of each centroid is moved to the mean of the points in the cluster\n",
    "        for label, cluster in self.clusters.items():\n",
    "            # compute the mean only if the cluster is not empty\n",
    "            if cluster:\n",
    "                self.centroids[label] = np.mean(cluster, axis=0)\n",
    "\n",
    "        # compute inertia\n",
    "        self._compute_inertia()\n",
    "\n",
    "        # save current centroids and clusters to history\n",
    "        self.history[i] = {'centroids': dict(self.centroids), 'clusters': dict(self.clusters), 'inertia': self.inertia}\n",
    "\n",
    "        # check if the algorithm has converged\n",
    "        converged = True\n",
    "        for label, centroid in self.centroids.items():\n",
    "            previous_centroid = previous_centroids[label]\n",
    "            centroid_distance = distance.euclidean(centroid, previous_centroid)\n",
    "            # if the distance between the current and previous centroid is greater than zero\n",
    "            # then the algorithm hasn't converged yet\n",
    "            if centroid_distance > 0:\n",
    "                converged = False\n",
    "\n",
    "        # if the algorithm converged then the learning process is finished\n",
    "        if converged:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Alguns métodos auxiliares são chamados durante o treinamento do modelo, são eles:\n",
    "- data_init: realiza a inicialização dos centroides com base no método escolhido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _data_init(self, data):\n",
    "    if self.init == 'kmeans++':\n",
    "        self._kmeans_plus_plus_init(data)\n",
    "    else:\n",
    "        self._forgy_init(data)\n",
    "\n",
    "def _forgy_init(self, data):\n",
    "    # chooses k random points from the data as initial centroids\n",
    "    np.random.seed(self.random_seed)\n",
    "    random_centroids = np.random.choice(len(data), self.k, replace=False)\n",
    "\n",
    "    for i in range(self.k):\n",
    "        self.centroids[i] = data[random_centroids[i]]\n",
    "\n",
    "def _kmeans_plus_plus_init(self, data):\n",
    "    np.random.seed(self.random_seed)\n",
    "    # the first centroid is chosen at random\n",
    "    centroids = [data[np.random.choice(len(data))]]\n",
    "\n",
    "    for i in range(self.k - 1):\n",
    "        dx_array = []\n",
    "        for data_point in data:\n",
    "            centroid_distances = []\n",
    "            # compute the distance from the data point to each centroid\n",
    "            for centroid in centroids:\n",
    "                centroid_distance = distance.euclidean(data_point, centroid)\n",
    "                centroid_distances.append(centroid_distance)\n",
    "\n",
    "            # dx denotes the square of the shortest distance from the data point to a centroid\n",
    "            dx = np.min(centroid_distances) ** 2\n",
    "            dx_array.append(dx)\n",
    "\n",
    "        # compute the probabilities\n",
    "        square_sum = np.sum(dx_array)\n",
    "        probabilities = np.divide(dx_array, square_sum)\n",
    "        # the point with the highest probability is the new centroid\n",
    "        highest_probability = np.max(probabilities)\n",
    "        new_centroid = data[np.where(probabilities == highest_probability)][0]\n",
    "        centroids.append(new_centroid)\n",
    "\n",
    "    for i in range(self.k):\n",
    "        self.centroids[i] = centroids[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- centroid_distance: dado um ponto pertencente ao conjunto de dados,\n",
    "retorna uma lista contendo as distâncias do ponto a cada um dos centroides definidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _centroid_distance(self, data_point):\n",
    "        # compute the euclidean distance from the data point to each centroid\n",
    "        distances = []\n",
    "        for _, centroid in self.centroids.items():\n",
    "            centroid_distance = distance.euclidean(data_point, centroid)\n",
    "            distances.append(centroid_distance)\n",
    "\n",
    "        return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- compute_inertia: calcula a inércia, medida definida como a soma do quadrado das distâncias de cada ponto ao seu centroide mais próximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _compute_inertia(self):\n",
    "        # compute the inertia (sum of squared distances of samples to their closest centroid)\n",
    "        inertia = 0\n",
    "        for label, centroid in self.centroids.items():\n",
    "            for point in self.clusters[label]:\n",
    "                error = distance.euclidean(centroid, point)\n",
    "                inertia += error ** 2\n",
    "        self.inertia = inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "O segundo método principal utilizado pelo algoritmo é o predict, que tem como objetivo realizar a predição de novos dados,\n",
    " ou seja, atribuir cada registro pertencente aos dados a algum dos clusters encontrados durante o treinamento.<br>\n",
    "Para isso, é utilizado o mesmo método para cálculo de distâncias entre pontos e centroides utilizado durante o treinamento.\n",
    "Portanto, cada registro é atribuido ao cluster para qual sua distância seja mínima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(self, data):\n",
    "        predict_data = []\n",
    "        for data_point in data:\n",
    "            # the data point is assigned to the nearest cluster\n",
    "            distances = self._centroid_distance(data_point)\n",
    "            label = np.argmin(distances)\n",
    "            predict_data.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - b) DBScan\n",
    "\n",
    "Esse algoritmo possui dois métodos principais, seguindo o padrão utilizado pela biblioteca scikit-learn, esses compreendem <i>fit</i> e <i>predict</i>. O primeiro tem o intuito de treinar o modelo, ou seja, identificar o comportamento dos registros. Os registros são iterados e classificados de acordo com <i>outlier</i>, borda ou central. Além disso, caso seja um registro central é atribuído a um novo cluster. As variáveis nc (node classification) e ci (cluster id) armazenam essas informações. A seguir, é descrito este método com comentários na língua inglesa para facilitar a extensão a partir da disponibilização do código implementado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, x):\n",
    "\n",
    "    # Initialize with 0's\n",
    "    nc = [0] * len(x)\n",
    "    ci = [0] * len(x)\n",
    "\n",
    "    cluster_id = 1\n",
    "\n",
    "    # Iterate through all records\n",
    "    for i in tqdm(range(len(x))):\n",
    "\n",
    "        # If already classified, skip\n",
    "        if nc[i] != 0:\n",
    "            continue\n",
    "\n",
    "        # Get neighbors\n",
    "        neighbors = self._get_neighbors(x, i)\n",
    "\n",
    "        # Verify if it is an outlier\n",
    "        if len(neighbors) < self.min_neighbors:\n",
    "            nc[i] = -1\n",
    "            continue\n",
    "\n",
    "        # Core record\n",
    "        nc[i] = 2\n",
    "        ci[i] = cluster_id\n",
    "\n",
    "        # Iterate through each neighbor\n",
    "        indx = 0\n",
    "        while True:\n",
    "\n",
    "            # If list of neighbors ended\n",
    "            if indx == len(neighbors):\n",
    "                break\n",
    "\n",
    "            j = neighbors[indx]\n",
    "\n",
    "            # Verify if neighbor is classified as outlier\n",
    "            if nc[j] == -1:\n",
    "                nc[j] = 1\n",
    "                ci[j] = cluster_id\n",
    "\n",
    "            # Verify if neighbor was already classified\n",
    "            if nc[j] != 0:\n",
    "                indx += 1\n",
    "                continue\n",
    "\n",
    "            post_neighbors = self._get_neighbors(x, j)\n",
    "\n",
    "            # Verify if neighbor is core point\n",
    "            if len(post_neighbors) >= self.min_neighbors:\n",
    "\n",
    "                nc[j] = 2\n",
    "                ci[j] = cluster_id\n",
    "\n",
    "            else:\n",
    "                # Classify as border point\n",
    "                nc[j] = 1\n",
    "                ci[j] = cluster_id\n",
    "\n",
    "            # Continue exploring neighbourhood\n",
    "            neighbors.extend(post_neighbors)\n",
    "            neighbors = list(set(neighbors))\n",
    "\n",
    "            indx += 1\n",
    "\n",
    "        cluster_id += 1\n",
    "\n",
    "    return (nc, ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível observar algumas chamadas para o método <i>_get_neighbors<i>. Este método tem o intuito de buscar todos os vizinhos de um determinado registro. Um método chamado <i>_verify_neighbor</i> faz o cálculo da distância euclidiana entre dois registros e retorna <i>True</i> se for vizinho, e <i>False</i> se não for vizinho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _verify_neighbor(self, point_a, point_b):\n",
    "\n",
    "    # Calculate euclidean distance\n",
    "    calc_dist = distance.euclidean(point_a, point_b)\n",
    "\n",
    "    # Append distance to verify description\n",
    "    self.summed_dist.append(calc_dist)\n",
    "\n",
    "    # Verify if it is a neighbor\n",
    "    if calc_dist <= self.distance:\n",
    "        return True, self.distance\n",
    "\n",
    "    return False, self.distance\n",
    "\n",
    "\n",
    "def _get_neighbors(self, x, i):\n",
    "    \n",
    "    # Neighbor list\n",
    "    neighbors = []\n",
    "\n",
    "    for j in range(len(x)):\n",
    "\n",
    "        if i == j:\n",
    "            continue\n",
    "\n",
    "        # Verify if it is a neighbor\n",
    "        verif, _ = self._verify_neighbor(x[i], x[j])\n",
    "        \n",
    "        if verif:\n",
    "            # Append to the list of neighbors\n",
    "            neighbors.append(j)\n",
    "\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora será descrito o método <i>predict</i> que faz a predição dos clusters para novos registros considerando os registros centrais já identificados. O método faz a identificação de qual é o registro central mais próximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x, res, y):\n",
    "    \n",
    "    (nc, ci) = res\n",
    "    \n",
    "    # Initialize with 0's\n",
    "    ci_pred = [0] * len(y)\n",
    "\n",
    "    for i in tqdm(range(len(y))):\n",
    "\n",
    "        # Get neighbors\n",
    "        neighbors = self._get_neighbors_predict(x, i, y)\n",
    "\n",
    "        if len(neighbors) > 0:\n",
    "\n",
    "            # Get closest neighbors\n",
    "            neighbors = self._get_by_closest(neighbors)\n",
    "\n",
    "            for j in range(len(neighbors)):\n",
    "                \n",
    "                # Get closest neighbors\n",
    "                indx_j = int(neighbors[j][0])\n",
    "\n",
    "                # Verify if core point\n",
    "                if nc[indx_j] == 2:\n",
    "                    ci_pred[i] = ci[indx_j]\n",
    "                    break\n",
    "\n",
    "    return ci_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método utiliza um método distinto para obter os vizinhos, pois é necessário considerar cada vizinho, como também, as distâncias para os vizinhos. O método tem nome <i>_get_neighbors_predict</i>. Um outro método, chamado <i>_get_by_closest</i>, é utilizado para ordenar todos os vizinhos de acordo com a distância calculada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_neighbors_predict(self, x, i, y):\n",
    "\n",
    "    # Neighbor list\n",
    "    neighbors = []\n",
    "\n",
    "    for j in range(len(x)):\n",
    "\n",
    "        # Verify if it is a neighbor\n",
    "        verif, dist = self._verify_neighbor(y[i], x[j])\n",
    "\n",
    "        if verif:\n",
    "            neighbors.append((j, dist))\n",
    "\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def _get_by_closest(self, neighbors):\n",
    "\n",
    "    neighbors = np.array(neighbors)\n",
    "    return neighbors[neighbors[:, 1].argsort()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - c) Principal Componente Analysis\n",
    "\n",
    "O algoritmo do PCA foi implementado utilizando a biblioteca [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). De acordo com a [wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis) o \"<i>Principal component analysis (PCA) is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest</i>\". Sendo assim, caso seja necessário reduzir um conjunto de dados para 3 dimensões é necessário apenas obter os 3 principais componentes. Para facilitar a utilização, foi criada uma classe nova chamada <i>OurPCA</i> com o método <i>fit_transform</i>, o qual cria um objeto PCA e faz a transformações nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class OurPCA:\n",
    "\n",
    "    def fit_transform(self, data, n_components):\n",
    "\n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        return pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além das classes e métodos descritos, outros algoritmos foram implementados para gerenciar os dados, resultados, e experimentos. Estes compreendem:\n",
    "- <i>main.py</i><br>\n",
    "Une tudo o que foi implementado para executar os experimentos.<br>\n",
    "\n",
    "\n",
    "- <i>inout.py</i><br>\n",
    "Encapsula métodos de leitura de arquivos, persistência de resultados, transformações nos dados, e criação de gráficos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III - Metodologia de Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - a) Bases de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste trabalho são utilizadas duas bases de dados. A primeira foi disponibilizada pela professora Esther, e possui 2 <i>features</i> numéricas. Para descrever os registros da base de dados o método <i>describe</i> da biblioteca pandas é utilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>573.000000</td>\n",
       "      <td>573.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1849.808028</td>\n",
       "      <td>15.227836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>900.129972</td>\n",
       "      <td>8.292268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>335.000000</td>\n",
       "      <td>1.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1155.000000</td>\n",
       "      <td>7.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1655.000000</td>\n",
       "      <td>17.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2350.000000</td>\n",
       "      <td>22.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3635.000000</td>\n",
       "      <td>29.150000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 x           y\n",
       "count   573.000000  573.000000\n",
       "mean   1849.808028   15.227836\n",
       "std     900.129972    8.292268\n",
       "min     335.000000    1.950000\n",
       "25%    1155.000000    7.450000\n",
       "50%    1655.000000   17.200000\n",
       "75%    2350.000000   22.750000\n",
       "max    3635.000000   29.150000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/cluster.dat', sep=' ')\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A segunda base de dados se refere a registros de históricos de cartões de crédito. O intuito da tarefa de agrupamento é identificar perfis de usuários para campanhas de marketing. Essa base de dados foi obtida do [Kaggle](https://www.kaggle.com/arjunbhasin2013/ccdata). Ela possui 18 <i>features</i> numéricas com alguns valores nulos e 8950 registros. Os valores nulos são preenchidos com 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BALANCE</th>\n",
       "      <th>BALANCE_FREQUENCY</th>\n",
       "      <th>PURCHASES</th>\n",
       "      <th>ONEOFF_PURCHASES</th>\n",
       "      <th>INSTALLMENTS_PURCHASES</th>\n",
       "      <th>CASH_ADVANCE</th>\n",
       "      <th>PURCHASES_FREQUENCY</th>\n",
       "      <th>ONEOFF_PURCHASES_FREQUENCY</th>\n",
       "      <th>PURCHASES_INSTALLMENTS_FREQUENCY</th>\n",
       "      <th>CASH_ADVANCE_FREQUENCY</th>\n",
       "      <th>CASH_ADVANCE_TRX</th>\n",
       "      <th>PURCHASES_TRX</th>\n",
       "      <th>CREDIT_LIMIT</th>\n",
       "      <th>PAYMENTS</th>\n",
       "      <th>MINIMUM_PAYMENTS</th>\n",
       "      <th>PRC_FULL_PAYMENT</th>\n",
       "      <th>TENURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8.950000e+03</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8949.000000</td>\n",
       "      <td>8.950000e+03</td>\n",
       "      <td>8.637000e+03</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1620.986304</td>\n",
       "      <td>7.714003</td>\n",
       "      <td>1003.204834</td>\n",
       "      <td>592.437371</td>\n",
       "      <td>411.067645</td>\n",
       "      <td>3.270640e+03</td>\n",
       "      <td>4.452865</td>\n",
       "      <td>2.030237</td>\n",
       "      <td>4.117664</td>\n",
       "      <td>2.214069</td>\n",
       "      <td>3.248827</td>\n",
       "      <td>14.709832</td>\n",
       "      <td>4494.449450</td>\n",
       "      <td>5.404793e+03</td>\n",
       "      <td>1.809551e+03</td>\n",
       "      <td>3.809273</td>\n",
       "      <td>11.517318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4385.370311</td>\n",
       "      <td>73.859272</td>\n",
       "      <td>2136.634782</td>\n",
       "      <td>1659.887917</td>\n",
       "      <td>904.338115</td>\n",
       "      <td>1.311675e+05</td>\n",
       "      <td>52.604114</td>\n",
       "      <td>29.744300</td>\n",
       "      <td>54.021898</td>\n",
       "      <td>32.210553</td>\n",
       "      <td>6.824647</td>\n",
       "      <td>24.857649</td>\n",
       "      <td>3638.815725</td>\n",
       "      <td>1.702333e+05</td>\n",
       "      <td>3.856941e+04</td>\n",
       "      <td>47.047820</td>\n",
       "      <td>1.338331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.916300e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>128.281915</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>39.635000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>3.833158e+02</td>\n",
       "      <td>1.692297e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>874.387676</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>361.280000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>8.577677e+02</td>\n",
       "      <td>3.128075e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2056.395445</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1110.130000</td>\n",
       "      <td>577.405000</td>\n",
       "      <td>468.637500</td>\n",
       "      <td>1.114220e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>6500.000000</td>\n",
       "      <td>1.906756e+03</td>\n",
       "      <td>8.260137e+02</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>311357.000000</td>\n",
       "      <td>875.000000</td>\n",
       "      <td>49039.570000</td>\n",
       "      <td>40761.250000</td>\n",
       "      <td>22500.000000</td>\n",
       "      <td>1.024992e+07</td>\n",
       "      <td>875.000000</td>\n",
       "      <td>875.000000</td>\n",
       "      <td>875.000000</td>\n",
       "      <td>1125.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>1.089444e+07</td>\n",
       "      <td>2.559345e+06</td>\n",
       "      <td>875.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             BALANCE  BALANCE_FREQUENCY     PURCHASES  ONEOFF_PURCHASES  \\\n",
       "count    8950.000000        8950.000000   8950.000000       8950.000000   \n",
       "mean     1620.986304           7.714003   1003.204834        592.437371   \n",
       "std      4385.370311          73.859272   2136.634782       1659.887917   \n",
       "min         0.000000           0.000000      0.000000          0.000000   \n",
       "25%       128.281915           0.900000     39.635000          0.000000   \n",
       "50%       874.387676           1.000000    361.280000         38.000000   \n",
       "75%      2056.395445           1.000000   1110.130000        577.405000   \n",
       "max    311357.000000         875.000000  49039.570000      40761.250000   \n",
       "\n",
       "       INSTALLMENTS_PURCHASES  CASH_ADVANCE  PURCHASES_FREQUENCY  \\\n",
       "count             8950.000000  8.950000e+03          8950.000000   \n",
       "mean               411.067645  3.270640e+03             4.452865   \n",
       "std                904.338115  1.311675e+05            52.604114   \n",
       "min                  0.000000  0.000000e+00             0.000000   \n",
       "25%                  0.000000  0.000000e+00             0.083333   \n",
       "50%                 89.000000  0.000000e+00             0.500000   \n",
       "75%                468.637500  1.114220e+03             1.000000   \n",
       "max              22500.000000  1.024992e+07           875.000000   \n",
       "\n",
       "       ONEOFF_PURCHASES_FREQUENCY  PURCHASES_INSTALLMENTS_FREQUENCY  \\\n",
       "count                 8950.000000                       8950.000000   \n",
       "mean                     2.030237                          4.117664   \n",
       "std                     29.744300                         54.021898   \n",
       "min                      0.000000                          0.000000   \n",
       "25%                      0.000000                          0.000000   \n",
       "50%                      0.083333                          0.166667   \n",
       "75%                      0.333333                          0.750000   \n",
       "max                    875.000000                        875.000000   \n",
       "\n",
       "       CASH_ADVANCE_FREQUENCY  CASH_ADVANCE_TRX  PURCHASES_TRX  CREDIT_LIMIT  \\\n",
       "count             8950.000000       8950.000000    8950.000000   8949.000000   \n",
       "mean                 2.214069          3.248827      14.709832   4494.449450   \n",
       "std                 32.210553          6.824647      24.857649   3638.815725   \n",
       "min                  0.000000          0.000000       0.000000     50.000000   \n",
       "25%                  0.000000          0.000000       1.000000   1600.000000   \n",
       "50%                  0.000000          0.000000       7.000000   3000.000000   \n",
       "75%                  0.250000          4.000000      17.000000   6500.000000   \n",
       "max               1125.000000        123.000000     358.000000  30000.000000   \n",
       "\n",
       "           PAYMENTS  MINIMUM_PAYMENTS  PRC_FULL_PAYMENT       TENURE  \n",
       "count  8.950000e+03      8.637000e+03       8950.000000  8950.000000  \n",
       "mean   5.404793e+03      1.809551e+03          3.809273    11.517318  \n",
       "std    1.702333e+05      3.856941e+04         47.047820     1.338331  \n",
       "min    0.000000e+00      1.916300e-02          0.000000     6.000000  \n",
       "25%    3.833158e+02      1.692297e+02          0.000000    12.000000  \n",
       "50%    8.577677e+02      3.128075e+02          0.000000    12.000000  \n",
       "75%    1.906756e+03      8.260137e+02          0.166667    12.000000  \n",
       "max    1.089444e+07      2.559345e+06        875.000000    12.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/credit.csv')\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - b) Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV - Resultados\n",
    "### Experimento 1: KMeans, primeiro dataset\n",
    "<img src=\"plots/d0_elbow_kmeans.png\">\n",
    "<img src=\"plots/d0_silhouette_kmeans_2.png\">\n",
    "<img src=\"plots/d0_silhouette_kmeans_3.png\">\n",
    "<img src=\"plots/d0_silhouette_kmeans_4.png\">\n",
    "<img src=\"plots/d0_silhouette_kmeans_5.png\">\n",
    "<img src=\"plots/d0_silhouette_kmeans_6.png\">\n",
    "<img src=\"plots/d0_silhouette_kmeans_7.png\">\n",
    "<img src=\"plots/d0_silhouette_kmeans_8.png\">\n",
    "<img src=\"plots/d0_silhouette_kmeans_9.png\">\n",
    "<img src=\"plots/d0_silhouette_kmeans_10.png\">\n",
    "<img src=\"plots/d0_silhouette_kmeans_11.png\">\n",
    "A partir da análise do resultado da aplicação do método do cotovelo no primeiro dataset, verifica-se que 3 é o número de clusters\n",
    "mais provável para melhor representar os dados. Tal hipótese é reforçada pela análise dos resultados do método da silhueta, que também\n",
    "apresentou 3 como sendo o possível melhor número de clusters, visto que, dado o intervalo analisado,\n",
    "o coeficiente da silhueta é maior em K = 3, assumindo o valor de 0.6963021999511755.\n",
    "\n",
    "### Experimento 2: KMeans, primeiro dataset\n",
    "#### Utilizando inicialização forgy\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_forgy_1.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_forgy_2.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_forgy_3.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_forgy_4.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_forgy_5.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_forgy_6.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_forgy_7.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_forgy_8.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_forgy_9.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_forgy_10.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_forgy_11.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_forgy_12.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_forgy_13.png\">\n",
    "<img src=\"plots/d0_inter_cluster_distance_not_scaled_forgy.png\">\n",
    "<img src=\"plots/d0_intra_cluster_distance_not_scaled_kmeans.png\">\n",
    "<img src=\"plots/d0_clusters_predict_not_scaled_forgy.png\">\n",
    "\n",
    "\n",
    "#### Utilizando inicialização kmeans++\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_kmeans_1.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_kmeans_2.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_not_scaled_kmeans_3.png\">\n",
    "<img src=\"plots/d0_inter_cluster_distance_not_scaled_kmeans.png\">\n",
    "<img src=\"plots/d0_intra_cluster_distance_not_scaled_kmeans.png\">\n",
    "<img src=\"plots/d0_clusters_predict_not_scaled_kmeans.png\">\n",
    "\n",
    "Nota-se que, independente da inicialização, o algoritmo não foi capaz de separar os clusters adequadamente\n",
    "durante os experimentos realizados com dados não normalizados, entretanto, percebe-se que, apesar do resultado incorreto,\n",
    "o algoritmo foi convergiu mais rapidamente ao utilizar a inicialização kmeans++.\n",
    "Ademais, nota-se que a distância entre clusters apresentou um comportamento indesejado com ambas inicializações,\n",
    "no caso da inicialização forgy, a distância atingiu um máximo na iteração 6 porém diminuiu nas iterações seguintes, enquanto\n",
    "que com a inicialização kmeans++, a distância atingiu um máximo na primeira iteração, porém também caiu nas iterações seguintes.\n",
    "\n",
    "### Experimento 3: KMeans, primeiro dataset\n",
    "#### Utilizando inicialização forgy\n",
    "<img src=\"plots/d0_cluster_iteration_scaled_forgy_1.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_scaled_forgy_2.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_scaled_forgy_3.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_scaled_forgy_4.png\">\n",
    "<img src=\"plots/d0_inter_cluster_distance_scaled_forgy.png\">\n",
    "<img src=\"plots/d0_intra_cluster_distance_scaled_forgy.png\">\n",
    "<img src=\"plots/d0_clusters_predict_scaled_forgy.png\">\n",
    "\n",
    "#### Utilizando inicialização kmeans++\n",
    "<img src=\"plots/d0_cluster_iteration_scaled_kmeans_1.png\">\n",
    "<img src=\"plots/d0_cluster_iteration_scaled_kmeans_2.png\">\n",
    "<img src=\"plots/d0_inter_cluster_distance_scaled_kmeans.png\">\n",
    "<img src=\"plots/d0_intra_cluster_distance_scaled_kmeans.png\">\n",
    "<img src=\"plots/d0_clusters_predict_scaled_kmeans.png\">\n",
    "\n",
    "Diferentemente do experimento anterior, nota-se que, com os dados normalizados, o algoritmo conseguiu separar os clusters\n",
    "de forma adequada. Novamente, percebe-se que o algoritmo convergiu mais rapidamente ao utilizar a inicialização kmeans++,\n",
    "caso onde a convergência ocorreu em apenas 2 iterações, ou seja, a inicialização kmeans++ foi capaz de escolher os melhores\n",
    "centros para o conjunto de dados.\n",
    "Em relação as inter e intra-clusters, nota-se que, diferente do experimento anterior, a evolução ocorreu da forma esperada, ou seja,\n",
    "a distância inter-cluster aumentou e a distância intra-cluster diminuiu ao longo das iterações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V - Conclusões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI - Apêndice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red\">Links, figuras, etc</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "- Scikit-learn (https://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}