{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"unicamp.png\" width=\"150\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MO444/MC886 - Aprendizado de Máquina e Reconhecimento de Padrões\n",
    "\n",
    "Esse trabalho foi feito pelos seguintes membros:\n",
    "\n",
    "- Lucas Zanco Ladeira - 188951\n",
    "- Rafael - \n",
    "\n",
    "O código original deste projeto está disponível em [repository inside Github](https://github.com/lucaslzl/p1_clustering). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Agrupamento e Redução de Dimensionalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Introdução\n",
    "\n",
    "Neste trabalho foi necessário implementar dois modelos de agrupamento e utilizar o algoritmo <b>Principal Component Analysis (PCA)</b> da biblioteca scikit-learn para a tarefa de redução de dimensionalidade. Os modelos implementados compreendem o <b>KMeans</b> e o <b>DBScan</b>. De forma resumida, no primeiro caso são selecionados centróides de clusters e analisados os clusters formados por esses centróides. O algoritmo é iterado um determinado número de vezes e clusters são encontrados na qual a distância entre os membros dos clusters e os centróides seja mínima dentro das iterações. No segundo caso, é calculada a distância entre todos os registros para encontrar quais formam clusters considerando uma distância máxima e uma vizinhança mínima. Os registros são classificados como <i>outlier</i>, borda, e central. Sendo assim, é um algoritmo interessante para identificar <i>outliers</i> dentro de conjuntos de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta seção será descrito o código fonte dos algoritmos implementados. Para tal, será sub-dividia em: a) KMeans, b) DBScan, c) PCA. Mesmo sendo que foi utilizada uma biblioteca para implementar o PCA, o código fonte será disponibilizado neste relatório."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - a) KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - b) DBScan\n",
    "\n",
    "Esse algoritmo possui dois métodos principais, seguindo o padrão utilizado pela biblioteca scikit-learn, esses compreendem <i>fit</i> e <i>predict</i>. O primeiro tem o intuito de treinar o modelo, ou seja, identificar o comportamento dos registros. Os registros são iterados e classificados de acordo com <i>outlier</i>, borda ou central. Além disso, caso seja um registro central é atribuído a um novo cluster. As variáveis nc (node classification) e ci (cluster id) armazenam essas informações. A seguir, é descrito este método com comentários na língua inglesa para facilitar a extensão a partir da disponibilização do código implementado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, x):\n",
    "\n",
    "    # Initialize with 0's\n",
    "    nc = [0] * len(x)\n",
    "    ci = [0] * len(x)\n",
    "\n",
    "    cluster_id = 0\n",
    "\n",
    "    # Iterate through all records\n",
    "    for i in tqdm(range(len(x))):\n",
    "\n",
    "        # If already classified, skip\n",
    "        if nc[i] != 0:\n",
    "            continue\n",
    "\n",
    "        # Get neighbors\n",
    "        neighbors = self._get_neighbors(x, i)\n",
    "\n",
    "        # Verify if it is an outlier\n",
    "        if len(neighbors) < self.min_neighbors:\n",
    "            nc[i] = -1\n",
    "            continue\n",
    "\n",
    "        cluster_id += 1\n",
    "\n",
    "        # Core record\n",
    "        nc[i] = 2\n",
    "        ci[i] = cluster_id\n",
    "\n",
    "        # Iterate through each neighbor\n",
    "        indx = 0\n",
    "        while True:\n",
    "\n",
    "            # If list of neighbors ended\n",
    "            if indx == len(neighbors):\n",
    "                break\n",
    "\n",
    "            j = neighbors[indx]\n",
    "\n",
    "            if i == j:\n",
    "                indx += 1\n",
    "                continue\n",
    "\n",
    "            # At least it is a border point\n",
    "            nc[j] = 1\n",
    "            ci[j] = cluster_id\n",
    "\n",
    "            post_neighbors = self._get_neighbors(x, j)\n",
    "\n",
    "            # Verify if neighbor is core point\n",
    "            if len(post_neighbors) >= self.min_neighbors:\n",
    "                # Classify as core point\n",
    "                nc[j] = 2\n",
    "\n",
    "                # Continue exploring neighbourhood\n",
    "                neighbors.extend(post_neighbors)\n",
    "                neighbors = list(set(neighbors))\n",
    "\n",
    "            indx += 1\n",
    "\n",
    "    return (nc, ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível observar algumas chamadas para o método <i>_get_neighbors<i>. Este método tem o intuito de buscar todos os vizinhos de um determinado registro. Um método chamado <i>_verify_neighbor</i> faz o cálculo da distância euclidiana entre dois registros e retorna <i>True</i> se for vizinho, e <i>False</i> se não for vizinho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _verify_neighbor(self, point_a, point_b):\n",
    "\n",
    "    # Calculate euclidean distance\n",
    "    calc_dist = distance.euclidean(point_a, point_b)\n",
    "\n",
    "    # Append distance to verify description\n",
    "    self.summed_dist.append(calc_dist)\n",
    "\n",
    "    # Verify if it is a neighbor\n",
    "    if calc_dist <= self.distance:\n",
    "        return True, self.distance\n",
    "\n",
    "    return False, self.distance\n",
    "\n",
    "\n",
    "def _get_neighbors(self, x, i):\n",
    "    \n",
    "    # Neighbor list\n",
    "    neighbors = []\n",
    "\n",
    "    for j in range(len(x)):\n",
    "\n",
    "        if i == j:\n",
    "            continue\n",
    "\n",
    "        # Verify if it is a neighbor\n",
    "        verif, _ = self._verify_neighbor(x[i], x[j])\n",
    "        \n",
    "        if verif:\n",
    "            # Append to the list of neighbors\n",
    "            neighbors.append(j)\n",
    "\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora será descrito o método <i>predict</i> que faz a predição dos clusters para novos registros considerando os registros centrais já identificados. O método faz a identificação de qual é o registro central mais próximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x, res, y):\n",
    "    \n",
    "    (nc, ci) = res\n",
    "    \n",
    "    # Initialize with 0's\n",
    "    ci_pred = [0] * len(y)\n",
    "\n",
    "    for i in tqdm(range(len(y))):\n",
    "\n",
    "        # Get neighbors\n",
    "        neighbors = self._get_neighbors_predict(x, i, y)\n",
    "\n",
    "        if len(neighbors) > 0:\n",
    "\n",
    "            # Get closest neighbors\n",
    "            neighbors = self._get_by_closest(neighbors)\n",
    "\n",
    "            for j in range(len(neighbors)):\n",
    "                \n",
    "                # Get closest neighbors\n",
    "                indx_j = int(neighbors[j][0])\n",
    "\n",
    "                # Verify if core point\n",
    "                if nc[indx_j] == 2:\n",
    "                    ci_pred[i] = ci[indx_j]\n",
    "                    break\n",
    "\n",
    "    return ci_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método utiliza um método distinto para obter os vizinhos, pois é necessário considerar cada vizinho, como também, as distâncias para os vizinhos. O método tem nome <i>_get_neighbors_predict</i>. Um outro método, chamado <i>_get_by_closest</i>, é utilizado para ordenar todos os vizinhos de acordo com a distância calculada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_neighbors_predict(self, x, i, y):\n",
    "\n",
    "    # Neighbor list\n",
    "    neighbors = []\n",
    "\n",
    "    for j in range(len(x)):\n",
    "\n",
    "        # Verify if it is a neighbor\n",
    "        verif, dist = self._verify_neighbor(y[i], x[j])\n",
    "\n",
    "        if verif:\n",
    "            neighbors.append((j, dist))\n",
    "\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def _get_by_closest(self, neighbors):\n",
    "\n",
    "    neighbors = np.array(neighbors)\n",
    "    return neighbors[neighbors[:, 1].argsort()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - c) Principal Componente Analysis\n",
    "\n",
    "O algoritmo do PCA foi implementado utilizando a biblioteca [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). De acordo com a [wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis) o \"<i>Principal component analysis (PCA) is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest</i>\". Sendo assim, caso seja necessário reduzir um conjunto de dados para 3 dimensões é necessário apenas obter os 3 principais componentes. Para facilitar a utilização, foi criada uma classe nova chamada <i>OurPCA</i> com o método <i>fit_transform</i>, o qual cria um objeto PCA e faz a transformações nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class OurPCA:\n",
    "\n",
    "    def fit_transform(self, data, n_components):\n",
    "\n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        return pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além das classes e métodos descritos, outros algoritmos foram implementados para gerenciar os dados, resultados, e experimentos. Estes compreendem:\n",
    "- <i>main.py</i><br>\n",
    "Une tudo o que foi implementado para executar os experimentos.<br>\n",
    "\n",
    "\n",
    "- <i>inout.py</i><br>\n",
    "Encapsula métodos de leitura de arquivos, persistência de resultados, transformações nos dados, e criação de gráficos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III - Metodologia de Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - a) Bases de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste trabalho são utilizadas duas bases de dados. A primeira foi disponibilizada pela professora Esther, e possui 2 <i>features</i> numéricas. Para descrever os registros da base de dados o método <i>describe</i> da biblioteca pandas é utilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>573.000000</td>\n",
       "      <td>573.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1849.808028</td>\n",
       "      <td>15.227836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>900.129972</td>\n",
       "      <td>8.292268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>335.000000</td>\n",
       "      <td>1.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1155.000000</td>\n",
       "      <td>7.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1655.000000</td>\n",
       "      <td>17.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2350.000000</td>\n",
       "      <td>22.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3635.000000</td>\n",
       "      <td>29.150000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 x           y\n",
       "count   573.000000  573.000000\n",
       "mean   1849.808028   15.227836\n",
       "std     900.129972    8.292268\n",
       "min     335.000000    1.950000\n",
       "25%    1155.000000    7.450000\n",
       "50%    1655.000000   17.200000\n",
       "75%    2350.000000   22.750000\n",
       "max    3635.000000   29.150000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/cluster.dat', sep=' ')\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A segunda base de dados se refere a registros de históricos de cartões de crédito. O intuito da tarefa de agrupamento é identificar perfis de usuários para campanhas de marketing. Essa base de dados foi obtida do [Kaggle](https://www.kaggle.com/arjunbhasin2013/ccdata). Ela possui 18 <i>features</i> numéricas com alguns valores nulos e 8950 registros. Os valores nulos são preenchidos com 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BALANCE</th>\n",
       "      <th>BALANCE_FREQUENCY</th>\n",
       "      <th>PURCHASES</th>\n",
       "      <th>ONEOFF_PURCHASES</th>\n",
       "      <th>INSTALLMENTS_PURCHASES</th>\n",
       "      <th>CASH_ADVANCE</th>\n",
       "      <th>PURCHASES_FREQUENCY</th>\n",
       "      <th>ONEOFF_PURCHASES_FREQUENCY</th>\n",
       "      <th>PURCHASES_INSTALLMENTS_FREQUENCY</th>\n",
       "      <th>CASH_ADVANCE_FREQUENCY</th>\n",
       "      <th>CASH_ADVANCE_TRX</th>\n",
       "      <th>PURCHASES_TRX</th>\n",
       "      <th>CREDIT_LIMIT</th>\n",
       "      <th>PAYMENTS</th>\n",
       "      <th>MINIMUM_PAYMENTS</th>\n",
       "      <th>PRC_FULL_PAYMENT</th>\n",
       "      <th>TENURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8.950000e+03</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8949.000000</td>\n",
       "      <td>8.950000e+03</td>\n",
       "      <td>8.637000e+03</td>\n",
       "      <td>8950.000000</td>\n",
       "      <td>8950.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1620.986304</td>\n",
       "      <td>7.714003</td>\n",
       "      <td>1003.204834</td>\n",
       "      <td>592.437371</td>\n",
       "      <td>411.067645</td>\n",
       "      <td>3.270640e+03</td>\n",
       "      <td>4.452865</td>\n",
       "      <td>2.030237</td>\n",
       "      <td>4.117664</td>\n",
       "      <td>2.214069</td>\n",
       "      <td>3.248827</td>\n",
       "      <td>14.709832</td>\n",
       "      <td>4494.449450</td>\n",
       "      <td>5.404793e+03</td>\n",
       "      <td>1.809551e+03</td>\n",
       "      <td>3.809273</td>\n",
       "      <td>11.517318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4385.370311</td>\n",
       "      <td>73.859272</td>\n",
       "      <td>2136.634782</td>\n",
       "      <td>1659.887917</td>\n",
       "      <td>904.338115</td>\n",
       "      <td>1.311675e+05</td>\n",
       "      <td>52.604114</td>\n",
       "      <td>29.744300</td>\n",
       "      <td>54.021898</td>\n",
       "      <td>32.210553</td>\n",
       "      <td>6.824647</td>\n",
       "      <td>24.857649</td>\n",
       "      <td>3638.815725</td>\n",
       "      <td>1.702333e+05</td>\n",
       "      <td>3.856941e+04</td>\n",
       "      <td>47.047820</td>\n",
       "      <td>1.338331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.916300e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>128.281915</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>39.635000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>3.833158e+02</td>\n",
       "      <td>1.692297e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>874.387676</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>361.280000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>8.577677e+02</td>\n",
       "      <td>3.128075e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2056.395445</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1110.130000</td>\n",
       "      <td>577.405000</td>\n",
       "      <td>468.637500</td>\n",
       "      <td>1.114220e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>6500.000000</td>\n",
       "      <td>1.906756e+03</td>\n",
       "      <td>8.260137e+02</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>311357.000000</td>\n",
       "      <td>875.000000</td>\n",
       "      <td>49039.570000</td>\n",
       "      <td>40761.250000</td>\n",
       "      <td>22500.000000</td>\n",
       "      <td>1.024992e+07</td>\n",
       "      <td>875.000000</td>\n",
       "      <td>875.000000</td>\n",
       "      <td>875.000000</td>\n",
       "      <td>1125.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>1.089444e+07</td>\n",
       "      <td>2.559345e+06</td>\n",
       "      <td>875.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             BALANCE  BALANCE_FREQUENCY     PURCHASES  ONEOFF_PURCHASES  \\\n",
       "count    8950.000000        8950.000000   8950.000000       8950.000000   \n",
       "mean     1620.986304           7.714003   1003.204834        592.437371   \n",
       "std      4385.370311          73.859272   2136.634782       1659.887917   \n",
       "min         0.000000           0.000000      0.000000          0.000000   \n",
       "25%       128.281915           0.900000     39.635000          0.000000   \n",
       "50%       874.387676           1.000000    361.280000         38.000000   \n",
       "75%      2056.395445           1.000000   1110.130000        577.405000   \n",
       "max    311357.000000         875.000000  49039.570000      40761.250000   \n",
       "\n",
       "       INSTALLMENTS_PURCHASES  CASH_ADVANCE  PURCHASES_FREQUENCY  \\\n",
       "count             8950.000000  8.950000e+03          8950.000000   \n",
       "mean               411.067645  3.270640e+03             4.452865   \n",
       "std                904.338115  1.311675e+05            52.604114   \n",
       "min                  0.000000  0.000000e+00             0.000000   \n",
       "25%                  0.000000  0.000000e+00             0.083333   \n",
       "50%                 89.000000  0.000000e+00             0.500000   \n",
       "75%                468.637500  1.114220e+03             1.000000   \n",
       "max              22500.000000  1.024992e+07           875.000000   \n",
       "\n",
       "       ONEOFF_PURCHASES_FREQUENCY  PURCHASES_INSTALLMENTS_FREQUENCY  \\\n",
       "count                 8950.000000                       8950.000000   \n",
       "mean                     2.030237                          4.117664   \n",
       "std                     29.744300                         54.021898   \n",
       "min                      0.000000                          0.000000   \n",
       "25%                      0.000000                          0.000000   \n",
       "50%                      0.083333                          0.166667   \n",
       "75%                      0.333333                          0.750000   \n",
       "max                    875.000000                        875.000000   \n",
       "\n",
       "       CASH_ADVANCE_FREQUENCY  CASH_ADVANCE_TRX  PURCHASES_TRX  CREDIT_LIMIT  \\\n",
       "count             8950.000000       8950.000000    8950.000000   8949.000000   \n",
       "mean                 2.214069          3.248827      14.709832   4494.449450   \n",
       "std                 32.210553          6.824647      24.857649   3638.815725   \n",
       "min                  0.000000          0.000000       0.000000     50.000000   \n",
       "25%                  0.000000          0.000000       1.000000   1600.000000   \n",
       "50%                  0.000000          0.000000       7.000000   3000.000000   \n",
       "75%                  0.250000          4.000000      17.000000   6500.000000   \n",
       "max               1125.000000        123.000000     358.000000  30000.000000   \n",
       "\n",
       "           PAYMENTS  MINIMUM_PAYMENTS  PRC_FULL_PAYMENT       TENURE  \n",
       "count  8.950000e+03      8.637000e+03       8950.000000  8950.000000  \n",
       "mean   5.404793e+03      1.809551e+03          3.809273    11.517318  \n",
       "std    1.702333e+05      3.856941e+04         47.047820     1.338331  \n",
       "min    0.000000e+00      1.916300e-02          0.000000     6.000000  \n",
       "25%    3.833158e+02      1.692297e+02          0.000000    12.000000  \n",
       "50%    8.577677e+02      3.128075e+02          0.000000    12.000000  \n",
       "75%    1.906756e+03      8.260137e+02          0.166667    12.000000  \n",
       "max    1.089444e+07      2.559345e+06        875.000000    12.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/credit.csv')\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III - b) Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível separar os experimentos em 2 grupos principais:\n",
    "- Modelos de agrupamento\n",
    "\n",
    "Nesse grupo são apresentados os resultados obtidos e é discutido o impacto de variar os hiperparâmetros dos modelos.\n",
    "\n",
    "\n",
    "- Redução de Dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV - Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV - a) Modelos de Agrupamento "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>DBScan - Distância Máxima</b>\n",
    "\n",
    "Para avaliar o DBScan é executado o treinamento do modelo em ambos os datasets variando a distância máxima e o número mínimo de vizinhos. Para encontrar um valor de distância que permita encontrar vizinhos, o algoritmo é executado e todas as distâncias calculadas são armazenadas. Com isso é obtida a média, mediana e moda das distâncias. Esse processo é executado para ambas as bases de dados sem/com a transformação de um <i>Scaler</i>. O <i>Scaler</i> utilizado faz a divisão de cada registro pelo valor máximo encontrado para cada <i>feature</i> da base de dados.\n",
    "\n",
    "Os diferentes valores resultantes encontrados permitem analisar a necessidade do <i>Scaler</i> para manter uma relação da distância entre cada <i>feature</i> distinta. No caso da utilização da primeira base de dados sem o <i>Scaler</i> foi possível obter os seguintes resultados apresentados a seguir. Encontramos que para a primeira base de dados, a segunda <i>feature</i> possui valor máximo de 29,15, mesmo assim, média da distância entre os pontos é de 1008,851 e mediana 845,234. Ou seja, a distância é afetada pela diferença de escala de ambas as <i>features</i>. Também é possível observar que a primeira <i>feature</i> chega ao valor de 3635,00.\n",
    "- Média: 1008,851 \n",
    "- Mediana: 845,234\n",
    "- Moda: 20,143\n",
    "\n",
    "Ao aplicar um <i>Scaler</i> em ambas as <i>features</i> da primeira base de dados encontramos os resultados apresentados a seguir. Como o <i>Scaler</i> muda a escala dos dados fazendo que ambos variem entre 0 e 1, existe uma consistência maior na diferença de distância. Os resultados encontrados para a média das distância é de 0,467, ou seja, é próximo do valor médio do intervalo da escala calculada.\n",
    "- Média: 0,467\n",
    "- Mediana: 0,530\n",
    "- Moda: 0,027\n",
    "\n",
    "Considerando a segunda base de dados sem o uso do <i>Scaler</i> foram encontrados os resultados apresentados a seguir. Nessa base de dados existem <i>features</i> com valor máximo de 2,55 até <i>features</i> com valor máximo de 311357,00. É possível observar que essas diferenças em escala impactam diretamente o algoritmo durante o uso da distância máxima tendo que a média resultante foi de 22313,913. Além disso, como essa base de dados possui uma quantidade maior de <i>features</i>, a diferença em escala impacta um maior número de variáveis.\n",
    "- Média: 22313,913\n",
    "- Mediana: 5427,695\n",
    "- Moda: 1019,243\n",
    "\n",
    "Ao aplicar um <i>Scaler</i> em todas as <i>features</i> foram encontrados os resultados apresentados a seguir. Nessa base de dados é possível observar que a média das distância foi de 0,259, ou seja, a maior parte das distâncias estão distribuídas no intervalo de 0 até 0,5. Observando os valores de cada <i>feature</i> que foram apresentados anteriormente junto com essa média, temos que o não uso do <i>Scaler</i> implica que a minoria das <i>features</i> iria impactar no resultado final do modelo.\n",
    "- Média: 0,259\n",
    "- Mediana: 0,192\n",
    "- Moda: 0,034\n",
    "\n",
    "Essas diferenças na escala dos dados e que resultam na distância, fazem com que o algoritmo ignore <i>features</i> com escalas menores ou que dê menos peso para essas <i>features</i>. Mesmo sendo que descrevemos os efeitos encontrados no DBScan, como o KMeans também é baseado na distância ele sofre dos mesmos problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>DBScan - Variação Hiperparâmetros</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V - Conclusões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI - Apêndice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red\">Links, figuras, etc</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "- Scikit-learn (https://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}